# ğŸ”’ Projeto 4: Distributed Lock

> Sistema de coordenaÃ§Ã£o crÃ­tico - aparece em 70% das entrevistas de system design

---

## ğŸ“‹ Problema

**DescriÃ§Ã£o:** Implementar distributed lock para coordenar acesso a recursos compartilhados em sistemas distribuÃ­dos.

**Problema Real:**
```
CenÃ¡rio: E-commerce com mÃºltiplos servidores

Servidor 1: Processar pedido #123 (estoque: 1 item)
Servidor 2: Processar pedido #124 (estoque: 1 item)

SEM LOCK:
  âœ— Ambos lÃªem estoque = 1
  âœ— Ambos confirmam pedido
  âœ— Estoque negativo! ğŸ’¥

COM LOCK:
  âœ“ Servidor 1 adquire lock
  âœ“ Servidor 1 processa pedido
  âœ“ Servidor 1 libera lock
  âœ“ Servidor 2 adquire lock
  âœ“ Servidor 2 vÃª estoque = 0
  âœ“ Servidor 2 rejeita pedido âœ…
```

---

## ğŸ¯ Requisitos

### Funcionais
1. âœ… `acquire()`: Adquirir lock (blocking ou non-blocking)
2. âœ… `release()`: Liberar lock
3. âœ… TTL automÃ¡tico (evitar deadlock se cliente crashar)
4. âœ… Reentrant lock (mesmo processo pode readquirir)

### NÃ£o-funcionais
1. **Mutual Exclusion**: Apenas 1 processo pode ter lock
2. **Deadlock Free**: Lock expira automaticamente
3. **Fault Tolerance**: Funciona mesmo se nÃ³ falhar
4. **Fairness**: Processos devem conseguir lock eventualmente

---

## ğŸ”§ ImplementaÃ§Ãµes

### 1. Database Lock (SIMPLES mas LIMITADO)

```python
# âŒ PROBLEMA: NÃ£o escala bem, single point of failure

from sqlalchemy import Column, Integer, String, DateTime
from datetime import datetime, timedelta

class DistributedLock(Base):
    __tablename__ = "distributed_locks"

    id = Column(Integer, primary_key=True)
    resource_name = Column(String(255), unique=True, index=True)
    locked_by = Column(String(255))
    locked_at = Column(DateTime)
    expires_at = Column(DateTime)


def acquire_lock(db, resource: str, client_id: str, ttl: int = 30) -> bool:
    """
    Tentar adquirir lock

    Retorna True se conseguiu, False caso contrÃ¡rio
    """
    now = datetime.utcnow()
    expires_at = now + timedelta(seconds=ttl)

    try:
        # Tentar inserir (lock disponÃ­vel)
        lock = DistributedLock(
            resource_name=resource,
            locked_by=client_id,
            locked_at=now,
            expires_at=expires_at
        )
        db.add(lock)
        db.commit()
        return True

    except IntegrityError:
        # Lock jÃ¡ existe, verificar se expirou
        db.rollback()

        lock = db.query(DistributedLock).filter(
            DistributedLock.resource_name == resource
        ).first()

        if lock and lock.expires_at < now:
            # Lock expirado, reaproveitar
            lock.locked_by = client_id
            lock.locked_at = now
            lock.expires_at = expires_at
            db.commit()
            return True

        return False


def release_lock(db, resource: str, client_id: str):
    """Liberar lock (apenas se vocÃª possui)"""
    db.query(DistributedLock).filter(
        DistributedLock.resource_name == resource,
        DistributedLock.locked_by == client_id
    ).delete()
    db.commit()


# Uso
import uuid

client_id = str(uuid.uuid4())

if acquire_lock(db, "process_order_123", client_id, ttl=30):
    try:
        # Critical section
        process_order(123)
    finally:
        release_lock(db, "process_order_123", client_id)
else:
    print("NÃ£o conseguiu lock, outro processo estÃ¡ executando")
```

**Problemas:**
- âŒ NÃ£o escala (database Ã© bottleneck)
- âŒ LatÃªncia alta (roundtrip ao DB)
- âŒ Single point of failure

---

### 2. Redis Lock (RECOMENDADO)

```python
import redis
import uuid
import time

class RedisLock:
    """
    Distributed Lock com Redis

    ImplementaÃ§Ã£o simples mas production-ready
    Usa SETNX (SET if Not eXists) + TTL
    """

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client

    def acquire(
        self,
        resource: str,
        ttl: int = 30,
        blocking: bool = True,
        timeout: int = 10
    ) -> str:
        """
        Adquirir lock

        Args:
            resource: Nome do recurso a lockear
            ttl: Time to live em segundos
            blocking: Se True, espera atÃ© conseguir lock
            timeout: Timeout para blocking mode

        Returns:
            Lock ID (UUID) se conseguiu, None caso contrÃ¡rio
        """
        lock_key = f"lock:{resource}"
        lock_id = str(uuid.uuid4())  # Identificador Ãºnico

        start_time = time.time()

        while True:
            # SET NX (Not eXists) + EX (EXpire)
            acquired = self.redis.set(
                lock_key,
                lock_id,
                nx=True,  # Apenas se nÃ£o existir
                ex=ttl    # Expira em N segundos
            )

            if acquired:
                return lock_id

            if not blocking:
                return None

            # Verificar timeout
            if time.time() - start_time > timeout:
                return None

            # Esperar um pouco antes de tentar novamente
            time.sleep(0.1)

    def release(self, resource: str, lock_id: str) -> bool:
        """
        Liberar lock (apenas se vocÃª possui)

        Usa Lua script para operaÃ§Ã£o atÃ´mica
        """
        lock_key = f"lock:{resource}"

        # Lua script para verificar e deletar atomicamente
        lua_script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
        """

        result = self.redis.eval(lua_script, 1, lock_key, lock_id)
        return bool(result)

    def extend(self, resource: str, lock_id: str, additional_ttl: int) -> bool:
        """
        Estender TTL do lock (Ãºtil para operaÃ§Ãµes longas)
        """
        lock_key = f"lock:{resource}"

        lua_script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("expire", KEYS[1], ARGV[2])
        else
            return 0
        end
        """

        result = self.redis.eval(lua_script, 1, lock_key, lock_id, additional_ttl)
        return bool(result)


# Uso bÃ¡sico
redis_client = redis.Redis(host='localhost', port=6379)
lock = RedisLock(redis_client)

lock_id = lock.acquire("process_order_123", ttl=30)

if lock_id:
    try:
        # Critical section
        process_order(123)
    finally:
        lock.release("process_order_123", lock_id)
else:
    print("NÃ£o conseguiu lock")


# Context manager (mais Pythonic)
from contextlib import contextmanager

@contextmanager
def redis_lock(redis_client, resource: str, ttl: int = 30):
    """Context manager para lock"""
    lock = RedisLock(redis_client)
    lock_id = lock.acquire(resource, ttl=ttl, blocking=True)

    try:
        yield lock_id
    finally:
        if lock_id:
            lock.release(resource, lock_id)


# Uso
with redis_lock(redis_client, "process_order_123", ttl=30):
    # Critical section
    process_order(123)
```

**AnÃ¡lise:**
- âœ… Muito rÃ¡pido (in-memory)
- âœ… TTL automÃ¡tico previne deadlock
- âœ… Lua script garante atomicidade
- âš ï¸ Single Redis node pode falhar (ver Redlock abaixo)

---

### 3. Redlock Algorithm (PRODUCTION-GRADE)

```python
import redis
import time
import uuid
from typing import List

class Redlock:
    """
    Redlock Algorithm (Martin Kleppmann, Redis Labs)

    Lock distribuÃ­do com mÃºltiplos Redis nodes
    Mais robusto contra falhas de nÃ³ individual
    """

    def __init__(self, redis_nodes: List[redis.Redis]):
        """
        Args:
            redis_nodes: Lista de Redis clients (mÃ­nimo 3, recomendado 5)
        """
        self.redis_nodes = redis_nodes
        self.quorum = len(redis_nodes) // 2 + 1  # Maioria

    def acquire(self, resource: str, ttl: int = 30, retry_count: int = 3) -> str:
        """
        Adquirir lock na maioria dos nÃ³s

        Algoritmo:
        1. Pegar timestamp atual
        2. Tentar adquirir lock em TODOS os nÃ³s
        3. Se conseguiu na MAIORIA (quorum) e dentro do tempo: sucesso
        4. Caso contrÃ¡rio: liberar locks e retry
        """
        lock_key = f"lock:{resource}"

        for _ in range(retry_count):
            lock_id = str(uuid.uuid4())
            start_time = time.time()

            # Tentar adquirir em todos os nÃ³s
            acquired_count = 0

            for node in self.redis_nodes:
                try:
                    success = node.set(lock_key, lock_id, nx=True, px=ttl * 1000)
                    if success:
                        acquired_count += 1
                except:
                    # Se um nÃ³ falhar, continuar tentando nos outros
                    pass

            # Calcular tempo de drift
            elapsed = time.time() - start_time
            validity_time = ttl - elapsed - 0.1  # 100ms drift

            # Verificar quorum
            if acquired_count >= self.quorum and validity_time > 0:
                return lock_id

            # NÃ£o conseguiu quorum, liberar todos os locks
            self._release_all(lock_key, lock_id)

            # Esperar random time antes de retry (evitar thundering herd)
            time.sleep(0.1 + (time.time() % 0.1))

        return None

    def release(self, resource: str, lock_id: str):
        """Liberar lock em todos os nÃ³s"""
        lock_key = f"lock:{resource}"
        self._release_all(lock_key, lock_id)

    def _release_all(self, lock_key: str, lock_id: str):
        """Helper: liberar lock em todos os nÃ³s"""
        lua_script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
        """

        for node in self.redis_nodes:
            try:
                node.eval(lua_script, 1, lock_key, lock_id)
            except:
                pass  # Ignorar falhas ao liberar


# Setup: 5 Redis nodes independentes
redis_nodes = [
    redis.Redis(host='redis1.example.com', port=6379),
    redis.Redis(host='redis2.example.com', port=6379),
    redis.Redis(host='redis3.example.com', port=6379),
    redis.Redis(host='redis4.example.com', port=6379),
    redis.Redis(host='redis5.example.com', port=6379),
]

redlock = Redlock(redis_nodes)

# Uso
lock_id = redlock.acquire("critical_resource", ttl=30)

if lock_id:
    try:
        # Critical section
        perform_critical_operation()
    finally:
        redlock.release("critical_resource", lock_id)
```

**AnÃ¡lise:**
- âœ… Fault tolerant (funciona se minoria dos nÃ³s falhar)
- âœ… No single point of failure
- âœ… Production-grade (usado por Redis Labs)
- âš ï¸ Mais complexo de setup e manter

---

### 4. ZooKeeper Lock (ENTERPRISE)

```python
from kazoo.client import KazooClient
from kazoo.recipe.lock import Lock

class ZooKeeperLock:
    """
    Distributed Lock com Apache ZooKeeper

    Usado por: Kafka, HBase, Hadoop
    Mais robusto que Redis para consensus
    """

    def __init__(self, hosts: str = "localhost:2181"):
        self.client = KazooClient(hosts=hosts)
        self.client.start()

    def acquire(self, resource: str, timeout: int = 30):
        """
        Adquirir lock

        ZooKeeper garante:
        - Mutual exclusion
        - Ordering (FIFO)
        - No starvation
        """
        lock_path = f"/locks/{resource}"
        lock = Lock(self.client, lock_path)

        acquired = lock.acquire(timeout=timeout)

        if acquired:
            return lock
        return None

    def release(self, lock: Lock):
        """Liberar lock"""
        lock.release()


# Uso
zk_lock = ZooKeeperLock(hosts="zk1:2181,zk2:2181,zk3:2181")

lock = zk_lock.acquire("critical_resource", timeout=30)

if lock:
    try:
        # Critical section
        perform_critical_operation()
    finally:
        zk_lock.release(lock)
```

**AnÃ¡lise:**
- âœ… Consensus protocol robusto (Zab)
- âœ… Ordering garantido (FIFO)
- âœ… No starvation
- âš ï¸ Mais pesado (Java-based)
- âš ï¸ Overhead maior que Redis

---

## ğŸš€ ComparaÃ§Ã£o

| CaracterÃ­stica | Database Lock | Redis Lock | Redlock | ZooKeeper |
|---------------|---------------|------------|---------|-----------|
| **LatÃªncia** | ğŸ”´ 100-200ms | ğŸŸ¢ 1-5ms | ğŸŸ¡ 5-15ms | ğŸŸ¡ 10-50ms |
| **Throughput** | ğŸ”´ 100/s | ğŸŸ¢ 10k/s | ğŸŸ¡ 5k/s | ğŸŸ¡ 1k/s |
| **Fault Tolerance** | ğŸ”´ SPOF | ğŸŸ¡ Limitado | ğŸŸ¢ Excelente | ğŸŸ¢ Excelente |
| **Complexidade** | ğŸŸ¢ Simples | ğŸŸ¢ Simples | ğŸŸ¡ MÃ©dio | ğŸ”´ Alto |
| **Custo** | ğŸŸ¢ Baixo | ğŸŸ¢ Baixo | ğŸŸ¡ MÃ©dio | ğŸ”´ Alto |

**Quando usar cada:**

- **Database Lock**:
  - âœ… Proof of concept
  - âœ… <100 locks/s
  - âŒ NÃƒO use em produÃ§Ã£o de alta escala

- **Redis Lock (single node)**:
  - âœ… 80% dos casos
  - âœ… Alta performance
  - âœ… Simplicidade
  - âš ï¸ Aceita perder lock em falha do Redis (raro)

- **Redlock (multi-node)**:
  - âœ… OperaÃ§Ãµes crÃ­ticas (pagamentos, estoque)
  - âœ… NÃ£o pode perder lock NUNCA
  - âœ… TolerÃ¢ncia a falhas importante

- **ZooKeeper**:
  - âœ… Sistemas enterprise complexos
  - âœ… Quando jÃ¡ usa ZooKeeper (Kafka, HBase)
  - âœ… Precisa de ordering/FIFO garantido

---

## ğŸ¯ FastAPI Integration

```python
from fastapi import FastAPI, HTTPException, Depends
from redis import Redis
import uuid

app = FastAPI()

# Redis client
redis_client = Redis(host='localhost', port=6379, decode_responses=True)

# Dependency para lock
def get_redis_lock():
    return RedisLock(redis_client)


@app.post("/orders/{order_id}/process")
async def process_order(
    order_id: int,
    lock: RedisLock = Depends(get_redis_lock)
):
    """
    Processar pedido com lock distribuÃ­do

    Garante que apenas 1 servidor processa o pedido por vez
    """
    resource = f"order:{order_id}"
    lock_id = lock.acquire(resource, ttl=30, blocking=False)

    if not lock_id:
        raise HTTPException(
            status_code=409,
            detail="Order is being processed by another server"
        )

    try:
        # Critical section
        order = get_order(order_id)

        if order.status != "pending":
            raise HTTPException(400, "Order already processed")

        # Processar pedido
        charge_payment(order)
        update_inventory(order)
        send_confirmation_email(order)

        # Marcar como processado
        order.status = "completed"
        db.commit()

        return {"status": "success", "order_id": order_id}

    finally:
        lock.release(resource, lock_id)


@app.post("/inventory/{product_id}/reserve")
async def reserve_inventory(
    product_id: int,
    quantity: int,
    lock: RedisLock = Depends(get_redis_lock)
):
    """
    Reservar estoque com lock

    Evita race condition em estoques baixos
    """
    resource = f"inventory:{product_id}"

    lock_id = lock.acquire(resource, ttl=10, blocking=True, timeout=5)

    if not lock_id:
        raise HTTPException(503, "Could not acquire lock, try again")

    try:
        product = db.query(Product).filter(Product.id == product_id).first()

        if product.stock < quantity:
            raise HTTPException(400, "Insufficient stock")

        # Reservar estoque
        product.stock -= quantity
        db.commit()

        return {"status": "reserved", "remaining_stock": product.stock}

    finally:
        lock.release(resource, lock_id)
```

---

## ğŸ§ª Testes

```python
import pytest
import threading
import time

def test_redis_lock_mutual_exclusion():
    """Apenas 1 thread deve conseguir lock"""
    lock = RedisLock(redis_client)
    resource = "test_resource"

    acquired_count = 0
    lock_id1 = None
    lock_id2 = None

    def worker1():
        nonlocal lock_id1, acquired_count
        lock_id1 = lock.acquire(resource, ttl=5, blocking=False)
        if lock_id1:
            acquired_count += 1
            time.sleep(1)
            lock.release(resource, lock_id1)

    def worker2():
        nonlocal lock_id2, acquired_count
        time.sleep(0.1)  # Garantir que worker1 adquire primeiro
        lock_id2 = lock.acquire(resource, ttl=5, blocking=False)
        if lock_id2:
            acquired_count += 1

    t1 = threading.Thread(target=worker1)
    t2 = threading.Thread(target=worker2)

    t1.start()
    t2.start()

    t1.join()
    t2.join()

    # Apenas worker1 deve ter conseguido lock
    assert lock_id1 is not None
    assert lock_id2 is None
    assert acquired_count == 1


def test_redis_lock_ttl_expiration():
    """Lock deve expirar automaticamente"""
    lock = RedisLock(redis_client)
    resource = "test_ttl"

    # Adquirir lock com TTL curto
    lock_id1 = lock.acquire(resource, ttl=1)
    assert lock_id1 is not None

    # Tentar adquirir imediatamente (deve falhar)
    lock_id2 = lock.acquire(resource, ttl=1, blocking=False)
    assert lock_id2 is None

    # Esperar expirar
    time.sleep(1.5)

    # Agora deve conseguir
    lock_id3 = lock.acquire(resource, ttl=5, blocking=False)
    assert lock_id3 is not None

    lock.release(resource, lock_id3)


def test_redis_lock_release_wrong_owner():
    """NÃ£o deve conseguir liberar lock de outro processo"""
    lock = RedisLock(redis_client)
    resource = "test_owner"

    lock_id1 = lock.acquire(resource, ttl=10)

    # Tentar liberar com lock_id errado
    fake_lock_id = str(uuid.uuid4())
    released = lock.release(resource, fake_lock_id)

    assert not released

    # Verificar que lock ainda estÃ¡ ativo
    lock_id2 = lock.acquire(resource, ttl=1, blocking=False)
    assert lock_id2 is None

    # Liberar corretamente
    lock.release(resource, lock_id1)


def test_redlock_fault_tolerance():
    """Redlock deve funcionar mesmo se minoria falhar"""
    # Simular 5 nÃ³s, 2 vÃ£o falhar
    working_nodes = [redis.Redis(host='localhost', port=6379) for _ in range(3)]
    failing_nodes = [MockFailingRedis() for _ in range(2)]

    all_nodes = working_nodes + failing_nodes
    redlock = Redlock(all_nodes)

    # Deve conseguir lock (3/5 nÃ³s OK = quorum)
    lock_id = redlock.acquire("test_resource", ttl=30)

    assert lock_id is not None

    redlock.release("test_resource", lock_id)


class MockFailingRedis:
    """Mock de Redis que sempre falha"""
    def set(self, *args, **kwargs):
        raise Exception("Node failed")

    def eval(self, *args, **kwargs):
        raise Exception("Node failed")
```

---

## ğŸ¯ Perguntas da Entrevista

**Interviewer:** "Por que preciso de distributed lock? Database transaction nÃ£o resolve?"

**VocÃª:** "Database transaction resolve dentro de 1 banco de dados. Mas em sistemas distribuÃ­dos vocÃª pode ter: mÃºltiplos bancos, cache (Redis), message queues, chamadas externas (APIs de pagamento). Distributed lock coordena TODOS esses recursos, nÃ£o apenas o DB."

---

**Interviewer:** "O que acontece se cliente crashar com lock ativo?"

**VocÃª:** "Por isso usamos TTL (Time To Live). Lock expira automaticamente apÃ³s N segundos. Importante: TTL deve ser maior que tempo esperado da operaÃ§Ã£o + margem de seguranÃ§a. Se operaÃ§Ã£o Ã© muito longa, usar `extend()` para renovar lock."

---

**Interviewer:** "Redis single node vs Redlock, qual escolher?"

**VocÃª:** "Depende do custo de perder o lock:
- **Single node**: 99.9% dos casos. Se Redis cair, sistema fica indisponÃ­vel por minutos, mas nÃ£o corrompe dados.
- **Redlock**: OperaÃ§Ãµes crÃ­ticas onde perder lock = corrupÃ§Ã£o de dados (ex: dÃ©bito duplo em conta bancÃ¡ria). Vale a complexidade extra."

---

**Interviewer:** "Por que nÃ£o usar SELECT FOR UPDATE no banco?"

**VocÃª:** "SELECT FOR UPDATE funciona para lock de ROW especÃ­fica. Distributed lock funciona para QUALQUER recurso: arquivo, API externa, cron job, cache. AlÃ©m disso, SELECT FOR UPDATE segura conexÃ£o com DB durante toda a operaÃ§Ã£o, nÃ£o escala bem."

---

## âœ… Checklist da Entrevista

- [ ] Explicar o problema (race condition em distribuÃ­do)
- [ ] Mostrar exemplo concreto (estoque, pedido)
- [ ] Discutir abordagem ingÃªnua (database lock)
- [ ] Propor Redis lock com TTL
- [ ] Implementar acquire/release com Lua script
- [ ] Explicar Redlock para fault tolerance
- [ ] Mencionar ZooKeeper para casos enterprise
- [ ] Tratar edge cases (crash, TTL, wrong owner)
- [ ] Integrar com FastAPI

---

## ğŸ“Š Casos de Uso Reais

**Onde Distributed Lock Ã© usado:**

1. **E-commerce**: Reserva de estoque
2. **Banking**: TransaÃ§Ãµes financeiras
3. **Cron Jobs**: Garantir single execution
4. **Cache Warming**: Evitar cache stampede
5. **Rate Limiting**: Quota distribuÃ­da
6. **Leader Election**: Escolher master node
7. **Database Migration**: Rodar apenas em 1 pod

**Empresas que usam:**
- Amazon: Order processing
- Uber: Trip allocation
- Netflix: Job scheduling
- Stripe: Payment processing

---

**Conceito essencial para sistemas distribuÃ­dos! ğŸ”’**
